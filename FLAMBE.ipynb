{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats.distributions import norm\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class SimplexEnvironment:\n",
    "    def __init__(self, states=100, actions=20, bell_rank=10):\n",
    "        self.S = states\n",
    "        self.A = actions\n",
    "        self.d = bell_rank\n",
    "        self.initial_distrib = self.unif_over_states\n",
    "\n",
    "        self.P = softmax(np.random.uniform(low=-self.d, high=self.d, size=(self.S, self.A, self.d)) , axis=2)\n",
    "        self.U = softmax(np.random.uniform(-self.d, self.d, (self.S, self.d)), axis=0)\n",
    "    \n",
    "    \n",
    "    def Phi(self, s, a):\n",
    "        return self.P[s, a, :]\n",
    "    \n",
    "    def Mu(self, s):\n",
    "        return self.U[s, :].T # s entre 0 y S-1 \n",
    "\n",
    "    def T(self, s, a, s_):\n",
    "        return np.dot(self.Phi(s, a), self.Mu(s_))\n",
    "\n",
    "    def unif_over_states(self, states=100):\n",
    "        return np.random.randint(low=0, high=states)\n",
    "\n",
    "    def unif_over_actions(self, current_state, actions):\n",
    "        return np.random.randint(low=0, high=actions)\n",
    "\n",
    "    def next_step_distrib(self, s, a):\n",
    "        return np.dot(self.Phi(s,a), self.U.T)\n",
    "\n",
    "    def next_step_state(self, s, a):\n",
    "        return np.random.choice(self.S, p=self.next_step_distrib(s,a))\n",
    "    \n",
    "    def next_step_reward(self, s, a):\n",
    "        return norm.rvs()\n",
    "    \n",
    "    def next_step(self, s, a):\n",
    "        \"\"\" \n",
    "        Gives the next state and reword for the given actions. \n",
    "        If `s` is the absorving state (i.e s == self.S - 1. Just by convention)\n",
    "        Then this will return 0 as reward and a state following the initial state distrib.\n",
    "        \"\"\"\n",
    "        if s == self.S - 1:\n",
    "            return self.first_step(), 0\n",
    "        else:\n",
    "            return self.next_step_state(s, a), self.next_step_reward(s, a)\n",
    "    \n",
    "    def first_step(self):\n",
    "        return self.initial_distrib(self.S)\n",
    "    \n",
    "    def get_transitions(self):\n",
    "        transitions = []\n",
    "        for a in tqdm(range(self.A)):\n",
    "            transitions.append([self.P[:, a, :] @ self.U.T])\n",
    "        return np.vstack(transitions)\n",
    "        \n",
    "    def simulate_n_steps(self, n):\n",
    "        path = []\n",
    "        prev_s = self.first_step()\n",
    "\n",
    "        for _ in tqdm(range(n), desc=f\"Simulating {n} Steps\"):\n",
    "            a = self.unif_over_actions(prev_s, self.A)\n",
    "            s = self.next_step_state(prev_s, a)\n",
    "            path.append((prev_s, a, s))\n",
    "        \n",
    "        return path\n",
    "\n",
    "    def simulate_n_steps_policy(self, n, policy):\n",
    "        path = []\n",
    "        prev_s = self.first_step()\n",
    "\n",
    "        for _ in tqdm(range(n), desc=f\"Simulating {n} Steps and custom policy\"):\n",
    "            a = policy(prev_s, self.A)\n",
    "            s = self.next_step_state(prev_s, a)\n",
    "            path.append((prev_s, a, s))\n",
    "        \n",
    "        return path\n",
    "\n",
    "    def simulate_n_steps_rewards(self, n):\n",
    "        path = []\n",
    "        prev_s = self.first_step()\n",
    "\n",
    "        for i in tqdm(range(n), desc=f\"Simulating {n} Steps with rewards\"):\n",
    "            a = self.unif_over_actions(prev_s, self.A)\n",
    "            s, r = self.next_step(prev_s, a)\n",
    "            path.append((prev_s, a, s, r))\n",
    "        \n",
    "        return path\n",
    "\n",
    "    def simulate_n_steps_rewards_policy(self, n, policy=None):\n",
    "        path = []\n",
    "        prev_s = self.first_step()\n",
    "\n",
    "        for i in tqdm(range(n), desc=f\"Simulating {n} Steps with rewards and custom policy\"):\n",
    "            a = policy(prev_s, self.A)\n",
    "            s, r = self.next_step(prev_s, a)\n",
    "            path.append((prev_s, a, s, r))\n",
    "        \n",
    "        return path\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
