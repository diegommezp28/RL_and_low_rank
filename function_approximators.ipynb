{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 549.45it/s]\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "import os\n",
    "import torch\n",
    "from utils import SimplexEnvironment\n",
    "\n",
    "MDP = load(open( os.path.join(\"data\", \"MDP_1.bin\"), \"rb\" ))\n",
    "t = torch.from_numpy(MDP.get_transitions()).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 1000 10\n"
     ]
    }
   ],
   "source": [
    "print(MDP.A, MDP.S, MDP.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a Neural Network To approximate the embedding functions of our MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module): \n",
    "    def __init__(self, states, actions, d):\n",
    "        super(Net, self).__init__()\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.d = d\n",
    "    \n",
    "        self.l1 = nn.Linear(self.states + self.actions, 120)  #First Linear layers, Receives concat onehot enconding of state-action pair\n",
    "        self.l2 = nn.Linear(120, 60)\n",
    "        self.embedding = nn.Linear(60, self.d)\n",
    "\n",
    "        self.mu_weights = nn.Parameter(torch.rand((self.d, self.states)))\n",
    "\n",
    "    def encode_input(self, s, a):\n",
    "         \"\"\"\n",
    "        # Parameters:\n",
    "        s: State id or list like of state ids between 0 and self.states\n",
    "        a: Action id or list like of actions ids between 0 and self.actions\n",
    "\n",
    "        If s and a are list-like, both need to be the same lenght\n",
    "         \"\"\"\n",
    "         input_len = len(s) if hasattr(s, '__len__') else 1\n",
    "         actions_len = len(a) if hasattr(a, '__len__') else 1\n",
    "         assert input_len == actions_len, f\"The input lenghts do not coincide. Input States: {input_len}; Input Actions: {actions_len}\"\n",
    "\n",
    "         s_hot = F.one_hot(torch.tensor(s).view(input_len, 1), self.states).to(torch.float32)\n",
    "         a_hot = F.one_hot(torch.tensor(a).view(input_len, 1), self.actions).to(torch.float32)\n",
    "         x = torch.cat((s_hot, a_hot), dim=-1)# Concat one hot vectors \n",
    "         return x\n",
    "\n",
    "    def enconde_output(self, s):\n",
    "        input_len = len(s) if hasattr(s, '__len__') else 1\n",
    "        return F.one_hot(torch.tensor(s).view(input_len, 1), self.states).to(torch.float32)\n",
    "\n",
    "\n",
    "    def phi(self, s, a):\n",
    "        \"\"\"\n",
    "        # Parameters:\n",
    "        s: State id or list like of state ids between 0 and self.states\n",
    "        a: Action id or list like of actions ids between 0 and self.actions\n",
    "\n",
    "        If s and a are list-like, both need to be the same lenght\n",
    "         \"\"\"\n",
    "        \n",
    "        x = self.encode_input(s, a) \n",
    "        x = F.leaky_relu(self.l1(x), negative_slope=0.01)\n",
    "        x = F.leaky_relu(self.l2(x), negative_slope=0.01)\n",
    "        x = F.softmax(self.embedding(x), dim=-1) # Apply softmax row wise\n",
    "\n",
    "        return x\n",
    "\n",
    "    def mu(self):\n",
    "        return F.softmax(self.mu_weights, dim=-1)\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        \"\"\"\n",
    "        # Parameters:\n",
    "        s: State id between 0 and self.states - 1\n",
    "        a: Action id between 0 and self.actions - 1\n",
    "        \"\"\"\n",
    "        x = self.phi(s, a)\n",
    "        soft_mu = self.mu()\n",
    "\n",
    "        # We use bradcasting in here so the same parameters are used for every element of the batch\n",
    "        x = torch.matmul(x, soft_mu) # Mat multiplication of (1, 1, d) @ (batch_size, d, states) --> (batch_size, 1, states) # Distribution over states\n",
    "\n",
    "        return x\n",
    "\n",
    "net = Net(MDP.S, MDP.A, MDP.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 10])\n",
      "tensor([[[0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "s = [1]\n",
    "a = [1]\n",
    "input_len = len(s) if hasattr(s, '__len__') else 1\n",
    "\n",
    "\n",
    "s_hot = F.one_hot(torch.tensor(s).view(input_len, 1), 5).to(torch.float32)\n",
    "a_hot = F.one_hot(torch.tensor(a).view(input_len, 1), 5).to(torch.float32)\n",
    "x = torch.cat((s_hot, a_hot), dim=-1)\n",
    "\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 1000])\n"
     ]
    }
   ],
   "source": [
    "s = [30, 5]\n",
    "a = [10, 3]\n",
    "\n",
    "y = net.forward(s, a)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(y, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7659, -0.6957, -2.9609],\n",
      "        [-0.0528,  1.4539,  1.0764]])\n",
      "tensor([[0.4579, 0.4911, 0.0510],\n",
      "        [0.1162, 0.5243, 0.3595]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim=-1)\n",
    "\n",
    "input = torch.randn(2, 3)\n",
    "print(input)\n",
    "output = m(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(output, dim=(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor([1]), 10).size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dl_arch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ae4df1c5943cf4748546a76d45f227f53fb8acff87095bbb53afeb5cd17aec1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
